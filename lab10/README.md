## Описание работы
В данной практической работе №10 изучается производительность параллельных программ и влияние различных способов параллелизации на время выполнения. В ходе работы рассматриваются программы, выполняющиеся на CPU, GPU и в распределённой среде, а также гибридные варианты, использующие несколько вычислительных ресурсов одновременно. Работа состоит из четырёх заданий. Все задания выполнялись в Google colab, так как на моем локальном компьютере отсутствует видеокарта NVIDIA.

#### Задание 1. Анализ производительности CPU-параллельной программы (OpenMP)
В первой части была написана программа на C++ с использованием OpenMP. Программа обрабатывает большой массив данных и считает сумму, среднее значение и дисперсию. Были сделаны последовательная версия программы, параллельная версия с OpenMP, замер времени выполнения, проверка, как меняется скорость при увеличении числа потоков, анализ ускорения с использованием закона Амдала.

#### Задание 2. Оптимизация доступа к памяти на GPU (CUDA)
Во второй части была написана программа на CUDA для GPU. Были реализованы ядро с хорошим (коалесцированным) доступом к памяти, ядро с плохим доступом к памяти, stencil-алгоритм без оптимизаций, stencil-алгоритм с использованием shared memory, измерение времени работы ядер с помощью cudaEvent. Целью задания было показать, что способ доступа к памяти сильно влияет на скорость работы GPU.

#### Задание 3. Профилирование гибридного приложения CPU + GPU
В третьей части была реализована программа, где часть вычислений выполняется на GPU, а часть на CPU. В программе данные обрабатываются по частям (чанками), используется асинхронная передача данных, применяются несколько CUDA streams, используется pinned память на CPU, измеряется время копирования данных и вычислений. Это позволяет увидеть, где возникают задержки при работе CPU и GPU вместе.

#### Задание 4. Анализ масштабируемости распределённой программы (MPI) 
В четвёртой части была написана распределённая программа с использованием MPI. Программа считает сумму, минимум и максимум элементов массива. Было выполнено исследование strong scaling (размер задачи фиксирован), исследование weak scaling (размер задачи на процесс фиксирован), измерение времени вычислений и коммуникаций, сравнение операций MPI_Reduce и MPI_Allreduce.

## Вывод
В ходе выполнения работы было показано, что увеличение числа потоков на CPU не всегда приводит к ускорению программы. На GPU скорость сильно зависит от того, как потоки обращаются к памяти. Использование shared memory может дать выигрыш, но не во всех случаях. В гибридных программах значительная часть времени тратится на передачу данных между CPU и GPU. В MPI-программах при большом числе процессов заметно увеличивается время обмена данными.

## Ответы на контрольные вопросы
1. В чём отличие измерения времени выполнения от профилирования?
Измерение времени показывает, сколько программа работает в целом. Профилирование показывает, какие части программы работают дольше всего и где возникают задержки.

2. Какие виды узких мест характерны для CPU, GPU и распределённых программ?
CPU: малое количество ядер, медленный доступ к памяти, последовательный код. GPU: медленный доступ к глобальной памяти и неправильная организация потоков. Распределённые программы: обмен данными между процессами и синхронизация.

3. Почему увеличение числа потоков или процессов не всегда приводит к ускорению?
Потому что часть программы остаётся последовательной, появляется лишняя синхронизация, и потоки или процессы начинают конкурировать за ресурсы.

4. Как законы Амдала и Густафсона применяются при анализе масштабируемости?
Закон Амдала показывает, что ускорение ограничено последовательной частью программы. Закон Густафсона показывает, что при увеличении размера задачи эффективность может оставаться высокой.

5. Какие факторы наиболее критичны для производительности гибридных приложений?
Скорость передачи данных между CPU и GPU, возможность выполнять копирование и вычисления одновременно, а также правильное распределение работы между CPU и GPU.
